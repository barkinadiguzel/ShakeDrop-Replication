# ğŸ”« ShakeDrop-Replication PyTorch Implementation

This repository contains a replication of **ResNet with ShakeDrop Regularization** using PyTorch. The goal is to reproduce a **ResNet backbone** enhanced with **ShakeDrop**, which probabilistically perturbs residual branches to improve regularization and convergence.

- Implemented **ResNet** with ShakeDrop for deep residual learning.  
- Architecture follows:  
**Conv1 â†’ ResidualBlock1 â†’ ResidualBlock2 â†’ ResidualBlock3 â†’ ResidualBlock4 â†’ BN â†’ ReLU â†’ AvgPool â†’ Flatten â†’ FC**  
**Paper**: [ShakeDrop Regularization for Deep Residual Learning](https://arxiv.org/abs/1705.07485)

---

## ğŸ–¼ Overview â€“ ResNet with ShakeDrop

![Figure 1](images/figmix.jpg)  

- **Figure 1:** Standard ResNet residual connections in our ShakeDrop implementation. Each block adds its input to the output via identity or projection shortcuts. ShakeDrop is applied to residual paths during training, introducing stochastic perturbation.  

- **Figure 2:** Conceptual illustration of ShakeDrop perturbation. Probabilistic switches randomly perturb residual branches:  
  - Vanilla ResNet can converge but may get trapped in local minima.  
  - Single-branch Shake introduces strong perturbation, avoiding local minima but may fail to converge.  
  - ShakeDrop combines these strategies, enabling convergence to better minima while regularizing the network.  

> ShakeDrop-ResNet extends standard ResNet by applying stochastic perturbation on residual branches. This prevents overfitting, encourages exploration of better minima, and improves generalization across datasets.


---

## ğŸ— Project Structure

```bash
ShakeDrop-Replication/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_layer.py          # BN â†’ ReLU â†’ Conv2d
â”‚   â”‚   â”œâ”€â”€ residual_block.py      # Basic/Bottleneck + ShakeDrop
â”‚   â”‚   â”œâ”€â”€ shortcut_layer.py      # Identity / Projection shortcut
â”‚   â”‚   â”œâ”€â”€ shake_drop.py          # ShakeDrop layer: probabilistic perturbation
â”‚   â”‚   â”œâ”€â”€ pool_layers/
â”‚   â”‚   â”‚   â”œâ”€â”€ maxpool_layer.py   # MaxPooling
â”‚   â”‚   â”‚   â””â”€â”€ avgpool_layer.py   # Global Average Pooling
â”‚   â”‚   â”œâ”€â”€ flatten_layer.py       # Conv â†’ FC transition
â”‚   â”‚   â””â”€â”€ fc_layer.py            # Fully Connected Layer (1000 classes)
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ resnet_shakedrop.py    # Full ResNet backbone + ShakeDrop implementation
â”‚   â”‚
â”‚   â””â”€â”€ config.py                  # Hyperparameters: channels, block counts, Î±, Î², survival prob etc.
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## ğŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
